\documentclass{IEEEtran}
\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage[ngerman]{babel}
\usepackage{hyperref}

\addto\extrasngerman{\def\sectionautorefname{Kapitel}}

\title{Staukontrolle durch Active Queue Management}
\author{Thomas Fischer, Dominik Billing}
\specialpapernotice{Masterseminar Kommunikationssysteme\\
Lehr- und Forschungseinheit für Kommunikationssysteme und Systemprogrammierung\\
Ludwig-Maximilians-Universität München}

\begin{document}
\maketitle

\begin{abstract}
Bei konventioneller Staukontrolle in Routern werden Pakete nicht gleichmäßig fallengelassen, wodurch es zu einem großen Overhead kommt, der durch Flaschenhälse im Internet noch verstärkt wird. Zusätzlich können Datenströme, die besonders viele Daten übertragen, nicht sinnvoll begrenzt werden. Wir werden als Lösung für das Problem der Staukontrolle im Internet Active Queue Management herausarbeiten. Active Queue Management versucht Staus frühzeitig und zuverlässig zu erkennen und durch Fallenlassen oder Markieren von Paketen (ECN -- Explicit Congestion Notification) Staus zu verhindern. Dies wird von AQM-Algorithmen erreicht, indem die mittlere Pufferauslastung von Routern möglichst gering gehalten wird.

Um einen Überblick über aktuelle AQM-Algorithmen zu erhalten, werden wir die AQM-Methoden RED (Random Early Detection), BLUE und AVQ (Adaptive Virtual Queue) vorstellen und untereinander vergleichen. Das Prinzip von RED ist, Pakete bereits vor dem Überlaufen der Queue mit einer bestimmten Wahrscheinlichkeit fallen zu lassen, um Staus vor dem Entstehen zu verhindern. Die Wahrscheinlichkeiten berechnen sich dabei mittels der aktuellen Queue Auslastung. Bei BLUE werden die ankommenden Pakete auch mit einer Wahrscheinlichkeit schon vorher fallen gelassen, diese berechnet sich mittels der verlorenen Pakete. Bei AVQ wird die Kapazität einer virtuellen Queue als Entscheidung für das Fallenlassen herangezogen.
\end{abstract}

\section{Einführung und Motivation}\label{sec:Motivation}
Die Ende-zu-Ende (E2E) Staukontrollmechanismen von TCP sind mittlerweile ein kritischer Faktor der Robustheit des Internets. Das Internet wächst unaufhaltsam weiter, es gibt keine eng verknüpfte Netzgemeinschaft und nicht jeder Endkonten verwendet die E2E Staukontrolle für bestmöglichen Datenfluss. Da auch Anwendungsentwickler sich nicht darum kümmern, E2E Staukontrolle in ihre Internet-Anwendungen zu integrieren, muss das Netz selbst seine Ressourcennutzung kontrollieren \cite{Floyd1997}.

Mit der Entwicklung von immer leistungsfähigeren Prozessoren, Arbeitsspeichern und Festplatten ist die knappste Ressource in Netzen aktuell die Übertragungsrate. Das bedeutet, dass nicht alle eingehenden Daten verarbeitet und weiter versendet werden können. Wenn die Puffer eines Routers voll laufen, werden neu ankommende Pakete nach dem "`Drop Tail"' Prinzip direkt abgewiesen (siehe \autoref{fig:droptail}). Hier müssen in Situationen hohen Datenaufkommens Pakete mehrmals versandt werden, was zu Lasten der Netzgeschwindigkeit geht. Das Transportschicht Protokoll TCP (Transmission Control Protocol) erkennt genau dann einen Stau, wenn Pakete im Fluss fehlen, also vorher schon fallen gelassen wurden \cite{Graffi2007}.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/droptail.png}
	\caption{Das Prinzip von "`Drop Tail"': Erreicht ein neues Paket in einem System mit vollem Puffer, wird es fallengelassen \cite{Graffi2007}.}
	\label{fig:droptail}
\end{figure}

Die Problemstellung ist es folglich Mechanismen zu finden, die Staus in E2E Verbindungen kontrollieren und frühzeitig identifizieren, damit Staus vermieden werden.

Um eine möglichst gute E2E Staukontrolle zu erreichen, soll die durchschnittliche Pufferausnutzung klein gehalten werden \cite{Le2003}. Die Herausforderung besteht darin, dass an den Flaschenhälsen der potenzielle Stau erkannt wird und das komplette Netz darauf reagiert, indem beispielsweise die Sendegeschwindigkeit des Ursprungs eines Datenstroms drastisch reduziert wird \cite{Graffi2007}. Interaktive Datenströme wie beispielsweise telnet, Web-Browsen und der Transfer von Audio- und Video-Material können sehr anfällig für Paketverluste oder hohe Latenzen sein, die auftreten, wenn Pakete erneut verschickt werden müssen \cite{Ramakrishnan2001}.

Das folgende \autoref{sec:Staukontrolle} analysiert das Problem der Staukontrolle in Netzen und präsentiert Active Queue Management als Lösungsansatz für das Problem. Zusätzlich wird mit Explicit Congestion Notification (ECN) eine Alternative zum einfachen Fallenlassen von Paketen vorgestellt. Anschließend für \autoref{sec:AQM} AQM ein und definiert Voraussetzungen eines funktionierenden Algorithmus. Die Active Queue Management Algorithmen RED, BLUE und AVQ werden in Kapitel \autoref{sec:Algorithmen} vorgestellt und in \autoref{sec:Vergleich} miteinander verglichen. Im abschließenden \autoref{sec:Ausblick} erfolgt eine Zusammenfassung der Ergebnisse zusammen mit einem Ausblick auf zukünftige Entwicklungen und Forschungen sowie andere Ansätze zur Staukontrolle.

\section{Staukontrolle in Netzen}\label{sec:Staukontrolle} 

Alle Staukontrollmechanismen müssen sich mit denen von TCP, als dem wichtigsten Transportschicht Protokoll im Internet, gemessen an den Datenflüssen, vergleichen \cite{Crowcroft1998}. Neben \cite{Floyd1997} wird auch von Morris \cite{Morris1997} postuliert, dass die Effektivität von TCP mit zunehmender Anzahl an konkurrierender Datenströme nachlässt. Diese Probleme sind auch im Internet spürbar und es zeigt sich, dass das Internet einen gravierenden Performanceverlust dadurch erfährt. Die einfachste Lösung für diese Probleme wäre die radikale Vergrößerung von Routerpuffern, zusammen mit einer Begrenzung der Anzahl der Pakete jedes einzelnen Datenstroms individuell. Da dies aber nicht ausführbar ist, müssen Staukontrollalgorithmen implementiert werden, mit den folgenden Hauptzielen \cite{Morris1997}: 
\begin{itemize}
	\item Es soll eine hohe Ausnutzung von Flaschenhälsen wie Routern erreicht werden. 
	\item Der Überlauf von Flaschenhälsen und damit eine zeitliche Verzögerung durch Staus sowie ein hoher Paketverlust soll verhindert werden. 
	\item Die zur Verfügung stehende Übertragungsrate soll gleichmäßig zwischen konkurrierenden Datenströmen aufgeteilt werden.
\end{itemize}
Um diese gleichmäßige Verteilung zu erreichen, ist TCP standardmäßig mit dem "`Drop Tail"' Prinzip nicht geeignet. Deshalb müssen Methoden entwickelt werden, die Puffer in Routern anders abarbeiten und dennoch einfach zu verwenden sind \cite{Suter1998}.

Die Internet Architektur basiert auf einem verbindungslosen Transportsystem, auf dem TCP als Transportschicht Protokoll aufsetzt. Diese Architektur hat vor allem aufgrund des verbindungslosen Designs, der Flexibilität und der Robustheit viele Vorteile, allerdings auch folgende Nachteile \cite{Braden1998}:
\begin{itemize}
	\item Sorgfältiges Design ist von Nöten, um bei hoher Last einen guten Dienst zu leisten.
	\item Mangelnde Aufmerksamkeit auf die Dynamik der Paketweiterleitung kann dazu führen, dass Dienste nicht mehr korrekt funktionieren und schlimmstenfalls zu einem sogenannten "`internet meltdown"'.
\end{itemize}
Das Phänomen eines solchen Internet-Zusammenbruchs wurde während der ersten Wachstumsphase des Internets in den 1980er Jahren festgestellt und wird "`congestion collapse"' \cite{Nagle1984} genannt. Bereits 1986 wurden von Jacobson entwickelte Stauverhinderungsmechanismen für Hosts entwickelt, die auch aktuell noch einen "`congestion collapse"' verhindern \cite{Braden1998}.

Da das Internet seit dieser Zeit immer weiter wächst, wurde es offensichtlich, dass TCP Stauverhinderungsmechanismen \cite{Stevens1997} nicht unter allen Umständen ausreichend gute Dienste leisten. Das Hauptproblem liegt darin, dass von den Enden der Netze nur bedingt Kontrolle ausgeübt werden kann. Deshalb müssen auch in Routern Mechanismen angewendet werden, welche die Stauverhinderungsmechanismen der Endpunkte ergänzen. Hierbei muss man zwischen den folgenden zwei Klassen unterscheiden \cite{Braden1998}:
\begin{description}
	\item[Queue Management] Algorithmen verwalten die Länge von Paket-Puffern durch Fallenlassen von Paketen wenn nötig oder angemessen.
	\item[Scheduling] Algorithmen legen fest, welche Pakete als nächstes gesendet werden sollen und können primär dafür genutzt werden, die Zuweisung von Übertragungsrate zwischen den Datenströmen zu verwalten.
\end{description}
Ein guter Algorithmus vereint die Vorteile beider Arten in sich, dazu mehr in \autoref{sec:AQM}.

Nach Jain \cite{Jain1990} gibt es zwei Gründe, warum das Problem der Staukontrolle in Netzen sehr schwierig ist. Erstens gibt es Voraussetzungen für Staukontrollschemas, die es schwierig machen eine zufriedenstellende Lösung zu finden. Zweitens gibt es unzählige Netzregeln, die das Design eines Stauschemas beeinflussen. Das sind die Gründe, warum ein Schema, das für ein Netz entwickelt wurde, in einem anderen Netz nicht funktioniert. Grundbedingungen für Schemata zur Staukontrolle sind: 
\begin{itemize}
	\item Ein Schema muss einen kleiner Overhead haben.
	\item Alle Datenströme müssen gleich behandelt werden.
	\item Es muss schnell auf andere Situationen reagiert werden können.
	\item Es muss in schlechten Umgebungen funktionsfähig sein.
	\item Es muss für alle Benutzer optimal sein.
\end{itemize}

Um E2E Staukontrolle in Routern zu betreiben, muss nicht nur jeder einzelne Router auf sich allein gestellt seinen Puffer überwachen, sondern auch an die nächsten Router Informationen weiterleiten. Für diese Benachrichtigung könnten zwei reservierte Bits im IP-Header genutzt werden \cite{Graffi2007}. Mittels "`Explicit Congestion Notification"' (ECN) sollen Pakete mit der Staubenachrichtigung weiterversandt werden im Gegensatz zum einfachen Fallenlassen der Pakete. Auf diese Art und Weise werden vorangehende Router darüber informiert, dass es zu einem Stau gekommen ist und möglicherweise einzelne Pakete doppelt versandt werden müssen oder die Geschwindigkeit gedrosselt werden sollte. Die Hauptaufgabe von Routern ist es Pakete anhand der Vermittlungsschicht (meist IP-Adresse) weiterzuleiten, im Gegensatz zu Switches, die anhand der Sicherungsschicht (beispielsweise MAC-Adresse) Pakete weiterleiten. In kleinen Netzen ist es prinzipiell kein Problem Pakete einfach fallen zu lassen. Da im heutigen Internet E2E Verbindungen über sehr viele Router gehen können, kann es hier schon problematisch werden. ECN ist ein zuverlässiger Mechanismus, beinhaltet allerdings keine Methoden zur Erkennung von Staus \cite{Floyd1994}. Da es nicht vorhersehbar ist, wie hoch der Datenverkehr zukünftig sein wird, besteht das wirkliche Problem also darin, Staus frühzeitig und zuverlässig zu erkennen \cite{Jain1996}.

Als Lösung für das Problem der Staukontrolle wird von vielen Seiten "`Active Queue Management"' (AQM) vorgeschlagen. AQM hat das Ziel Staus in Netzen rechtzeitig zu entdecken, bevor die Routerpuffer volllaufen. Um den Quellen von Datenströmen zu signalisieren, dass es zu Problemen gekommen ist oder kommen wird, gibt es im Prinzip die beiden Möglichkeiten Pakete fallen zu lassen oder diese zu markieren. Die erste Strategie erfordert es, dass die Endpunkte kooperieren und erzeugt durch erneuten Versandt der Pakete einen Overhead, der bezüglich der verbrauchten Übertragungsrate messbar ist. Andererseits müssen Router auf markierte Pakete reagieren, als wären diese fallengelassen worden. Es wird hier allerdings kein weiterer Overhead erzeugt, da die Pakete nicht erneut gesendet werden müssen. AQM-Algorithmen können zusätzlich die Übertragungsrate von besonders gierigen Datenströmen reduzieren, indem deren Pakete häufiger fallengelassen oder markiert werden als andere. Einen wirklichen Unterschied machen AQM-Algorithmen allerdings nur, wenn sie wirklich flächendeckend und der Situation entsprechend eingesetzt werden \cite{Graffi2007}.

\section{Definition und Anwendung von Active Queue Management}\label{sec:AQM}

TCP ist ein E2E Protokoll, das zusammen mit dem Internet Protokoll (IP) genutzt wird, um Daten in Form von Paketen zwischen Computern über das Internet zu übertragen. Während IP für die Vermittlung der Daten sorgt, gehören Datensicherheit, Flusssteuerung und das Ergreifen von Maßnahmen bei Datenverlust zu den Aufgaben von TCP. TCP implementiert einen Algorithmus zur Flusssteuerung, der "`Sliding Window"' genannt wird. Das "`Window"' ist die Anzahl an Bytes, die gesendet werden können ohne auf Empfangsbestätigung warten zu müssen. Der Algorithmus besteht aus folgenden Schritten \cite{Tannenbaum2012}, \cite{Socolofsky1991}:
\begin{enumerate}
	\item Alle Bytes eines Windows verschicken.
	\item Auf die Empfangsbestätigung warten (es können mehrere Pakete auf einmal zurück gemeldet werden).
	\item Das Window bis an das Ende des lückenlos bestätigten Bytestroms verschieben.
	\item Wenn die Empfangsbestätigung eines Pakets nicht in einer angegebenen Zeit erfolgt, dann wird das Paket neu verschickt.
\end{enumerate}
TCP nimmt hier keine Rücksicht auf Staus im Netz. Wenn also kontinuierlich immer weiter Pakete an das Netz versandt werden, ohne dass diese vom Netz aufgenommen werden können, führt dies zu einem Stau, der sich eventuell immer weiter aufbaut. An genau dieser Stelle muss ein Active Queue Management Algorithmus ansetzen und die Staukontrolle übernehmen \cite{Oldak2006}.

Alle Internetrouter besitzen Puffer, um auf Verzögerungen im Fluss kurzfristig reagieren zu können und Daten eine kurze Zeit vorzuhalten. Die Größe von Routerpuffern ist durch Hardware festgelegt orientiert sich allerdings an der Dynamik des Staukontrollalgorithmus von TCP. Genauer gesagt ist es das Ziel einen Puffer nie leer laufen zu lassen. Die Größe eines Puffers muss nach dieser Definition mit wachsender Verbindungsgeschwindigkeit ansteigen. Das Problem hierbei ist, dass auf diese Art große und langsame Speicher verwendet werden müssten. Durch die Anwendung von besseren Staukontrollalgorithmen wie AQM können diese kostspieligen Puffer klein bleiben \cite{Appenzeller2004}.

Die Definition von Active Queue Management lautet wie folgt:
\begin{description}
	\item[Active Queue Management] (AQM) ist das aktive Neusortieren oder Fallenlassen von Paketen innerhalb eines Puffers.
\end{description}
Damit sind AQM Algorithmen eine Mischung aus Scheduling und Queue Management Algorithmen, mit folgenden Zielen \cite{Suter1998a}, \cite{Pan2008}:
\begin{itemize}
	\item Es sollen möglichst wenig Pakete fallengelassen werden.
	\item Daten aus einfachen Datenquellen wie beispielsweise telnet sollen mit einer sehr kurzen Verzögerung behandelt werden.
	\item Die Übertragungsrate soll gleichbehandelnd zwischen den unterschiedlichen Flüssen aufgeteilt werden.
	\item Staus sollen frühzeitig erkannt werden.
	\item Flüsse aus nicht responsiven Quellen sollen derart behandelt werden, dass responsive Flüsse davon nicht beeinträchtigt werden.
	\item Der Algorithmus soll einfach zu implementieren sein und schnell reagieren.
\end{itemize}
Das ist nur möglich, wenn sich alle Datenquellen untereinander derart koordinieren, dass die Queue-Größe unter Kontrolle gehalten wird. Das Problem hierbei besteht darin, dass Übertragungsrate, Schnelligkeit und Pufferplatz unabhängig von einander zugeordnet werden müssen\cite{Demers1989}.

Selbst wenn AQM Mechanismen verwendet werden, ist es oft sehr sinnvoll zusätzlich auch ECN zu verwenden, um einen unnötigen Overhead zu vermeiden. Bei ECN wird durch das Markieren eines Pakets, die Kommunikation zwischen einzelnen Routern im Netz ermöglicht. Zusammen haben AQM und ECN das Potenzial den Effekt von Verlusten in latenz-sensitiven Flüssen zu reduzieren \cite{Ramakrishnan2001}. Aus diesem Grund hängt der Erfolg jedes AQM Machanismus stark damit zusammen, ob er mit ECN verbunden werden kann und wie gut diese Verbindung erreicht wird \cite{Hollot2001}.

Im Laufe der Zeit ist sehr viel Aufwand in unterschiedliche AQM-Methoden und deren Feintuning gesteckt worden. Viele dieser Arbeiten basieren auf Heuristiken und Simulationen und nicht auf einem systematischen Ansatz. Ihr gemeinsames Problem ist, dass jede vorgeschlagene Konfiguration nur für ein spezielles Datenverkehrsaufkommen geeignet ist, aber nachteilige Effekte aufweist, wenn sie in einer anderen Umgebung angewendet wird. Entsprechend müssen AQM-Methoden entwickelt und ihre Parameter so eingestellt werden, dass sie in einer Vielzahl von unterschiedlichen Umgebungen einsetzbar sind und zumindest keine schlechteren Ergebnisse liefern als TCP selbst \cite{Firoiu2000}. Es existieren sehr viele unterschiedliche AQM-Methoden, die komplett unterschiedliche Ansätze verfolgen. Manche Algorithmen berechnen beispielsweise Wahrscheinlichkeiten, um Pakete fallen zu lassen, andere nutzen Verfahren aus der mathematischen Optimierung aus, um die bestmögliche Staukontrolle zu erreichen. Alle Algorithmen unterscheiden sich in der Anzahl an möglichen Einsatzgebieten, Komplexität und Qualität \cite{Hollot2001}.

Im nächsten Kapitel stellen wir drei der bekanntesten Algorithmen vor.

\section{Drei Beispiele für Active Queue Management Algorithmen}\label{sec:Algorithmen}

Seitdem die ersten Ideen für Active Queue Management vorgestellt wurden und der Einführung von ECN in TCP/IP wurden viele verschiedene AQM Algorithmen entwickelt. Es würde weit über den Rahmen dieser Arbeit hinausgehen, sie alle zu erwähnen, weshalb hier die gängigsten drei RED, BLUE und AVQ vorgestellt werden. Es wurden diese drei Algorithmen ausgewählt, da zu diesen sehr viele Arbeiten existieren und sie bei vielen anderen Algorithmen als Referenz dienen.

\subsection{RED: Random Early Detection}

Der erste Algorithmus, der präsentiert wird, ist "`Random Early Detection"' (RED). Er war einer der ersten AQM Algorithmen und viele andere Arbeiten entwickelten diesen weiter, z.B. in \cite{Floyd2001} oder \cite{Pan2000}. Algorithmen, die anders ablaufen, werden oft mit RED verglichen.

Floyd und Van Jacobson haben RED 1993 vorgestellt \cite{Floyd1993}. In ihrer Arbeit wird die Funktionsweise des Algorithmus dargestellt. Um die Sender über einen Stau zu informieren kann RED entweder Pakete fallen lassen oder das ECN-Bit im Header setzen, je nach Router. Im Folgenden werden wir nur die Möglichkeit des Markierens (ECN-Bit Setzen) betrachten, was auf den eigentlichen Algorithmus keinerlei Auswirkungen hat. 

Das Prinzip von RED ist es Pakete mit einer Wahrscheinlichkeit zu markieren, die sich proportional zum Anteil der Übertragungsrate verhält, den diese Verbindung belegt. Auf diese Weise versucht der Algorithmus die Ressourcenzuteilung fair zu machen. 

Als Messgröße wird die durchschnittliche Queue Länge benutzt. Die durchschnittliche Länge $Q_{avq}$ wird für jedes eintreffende Paket mittels der aktuellen Länge der Queue $q$ und dem Gewicht der Queue $w_{q}$ folgendermaßen neu berechnet: 
\begin{displaymath}
Q_{avg} = (1 - w_q) Q_{avg} + w_q q
\end{displaymath}
Dieser Wert wird mit zwei Parametern verglichen, der minimalen Queuelänge $Q_{min}$ und der maximalen Queuelänge $Q_{max}$. Ist \(Q_{min} > Q_{avg}\), so wird nichts unternommen. Wird aber \(Q_{min} < Q_{avg} < Q_{max}\), so wird das Paket mit einer Markierungswahrscheinlichkeit $p_a$ markiert, und sobald \(Q_{avg} > Q_{max}\) wird jedes Paket markiert.

Für die Berechnung der finalen Markierungswahrscheinlichkeit $p_{a}$ wird die Markierungswahrscheinlichkeit $p_{b}$ benötigt. Diese berechnet sich wie folgt aus der minimalen und maximalen Queuelänge $Q_{min}$ und $Q_{max}$, der Durchschnittslänge $Q_{avg}$ und dem Maximum für $p_{b}$, $max_{b}$, wie folgt: 
\begin{displaymath}
p_b = max_p \frac{Q_{avg} - Q_{min}}{Q_{max} - Q_{min}}
\end{displaymath}
$p_{b}$ steigt folglich linear von 0 bis zum Wert $max_{b}$ an. Die finale Markierungswahrscheinlichkeit wird mittels $p_{b}$ und eines Zählers $z$ berechnet:
\begin{displaymath}
p_a = \frac{p_b}{1-z p_b}
\end{displaymath}
Der Zähler wird für jedes einkommende Paket inkrementiert. Ein Paket wird mit der Wahrscheinlichkeit $p_{a}$ markiert. Ist dies der Fall, so wird der Zähler $z$ auf 0 gesetzt. Mithilfe des Zählers steigt die Wahrscheinlichkeit somit für jedes weitere Paket an.

Neben der Queuelänge in Paketen könnte RED auch die Bytelänge und somit die Anzahl an Bytes eines Pakets zur Bewertung heranziehen. Diese Information wird dann in die Markierungswahrscheinlichkeit $p_b$ mit einbezogen. Nach der Berechnung ändert sich der Wert dann in 
\begin{displaymath}
p_b = p_b \frac{Paketbytes}{maximale~Paketbytes}
\end{displaymath}
Dadurch werden große Pakete mit einer höheren Wahrscheinlichkeit markiert als kleine Pakete. An der Beschreibung des Algorithmus wird ersichtlich, dass RED viele Parameter benötigt, die vorab festgelegt werden müssen. Damit RED gut läuft, müssen für jeden Router die richtigen Werte gefunden werden, was ein nicht zu vernachlässigendes Problem des Algorithmus darstellt. Mehr dazu in \autoref{sec:Vergleich}.

\subsection{BLUE}

Der BLUE Algorithmus wurde 1999 von Feng et.al. an der University of Michigan in Zusammenarbeit mit IBM vorgestellt \cite{Feng1999}. BLUE wurde entwickelt, um einige Schwachstellen von RED zu verbessern, ist aber ein völlig neuer Ansatz. Wie bereits erläutert verlässt sich RED auf die Queuelänge, um Staus anzuzeigen, und benötigt viele Parameter, die konfiguriert werden müssen. Die Autoren von BLUE argumentieren, dass RED nur wenn diese richtig konfiguriert sind und wenn ausreichend Pufferplatz zur Verfügung steht optimal läuft. BLUE dagegen verlässt sich auf den Paketverlust und die Verbindungsauslastung um seine Markierungswahrscheinlichkeit zu berechnen. Genauso wie RED kann BLUE dann entweder Pakete fallen lassen oder mittels ECN markieren.

Der BLUE Algorithmus kennt nur eine Markierungswahrscheinlichkeit $p_m$; jedes einzureihende Paket wird mit dieser Wahrscheinlichkeit markiert. Die Entscheidung für die Erhöhung oder Erniedrigung dieser Wahrscheinlichkeit wird auf Basis der verlorenen Pakete beziehungsweise auf Basis der ungenutzten Verbindungen getroffen: Erhält der Router die Information, dass ein Paket verloren gegangen ist, wird $p_m$ um den Wert $d_1$ erhöht. Erkennt er eine ungenutzte Verbindung, wird $p_m$ um $d_2$ reduziert. Ein weiterer Parameter ist hierbei noch wichtig: die \textit{freeze\_time}. Damit das Netz und die Sender Zeit haben, auf die Aktion des Routers zu reagieren, muss mindestens dieses Zeitintervall vergangen sein, bis die Markierungswahrscheinlichkeit wieder geändert wird. Formal läuft der Algorithmus folgendermaßen ab:
\newline \(if(Paketverlust \wedge (now - last\_update) > freeze\_time)~then\)
\newline \indent \(p_m = p_m + d_1\)
\newline \indent \(last\_update = now\)
\newline \(if(Verbindung~frei \wedge (now - last\_update > freeze\_time))~then\)
\newline \indent \(p_m = p_m - d_2\)
\newline \indent \(last\_update = now\)

Die beiden Parameter $d_1$ und $d_2$ geben an, um wie viel $p_m$ zu erhöhen beziehungsweise zu reduzieren ist. $d_1$ sollte deutlich größer sein als $d_2$, da BLUE somit auf Staus sehr viel schneller reagieren kann. Die Autoren geben weiterhin an, dass in ihren Experimenten die minimale Zeitspanne zwischen Änderungen an $p_m$, die \textit{freeze\_time}, konstant gehalten wurde. Sie sagen aber auch, dass in einem Netz dieser Parameter zufällig für jeden Router gewählt werden sollte, um globale Synchronisation zu vermeiden. Dieser Algorithmus passt sich somit selbstständig an den aktuellen Bedarf des Netzes an und benötigt keine Router-abhängigen Parameter zur Konfiguration.

\subsection{AVQ: Adaptive Virtual Queue}

Ein weiterer AQM-Algorithmus, der eine andere Idee verfolgt, ist der Adaptive Virtual Queue (AVQ) Algorithmus. Er wurde 2001 von Kunniyur und Srikant vorgestellt \cite{Kunniyur2001}. Wie bereits der Name andeutet basiert der Algorithmus auf einer virtuellen Queue. Für die Markierung beziehungsweise das Fallenlassen von Paketen wird die Kapazität dieser virtuellen Queue und keine Markierungswahrscheinlichkeit zu Rate gezogen.

Bei AVQ verwaltet der Router neben der echten Queue eine virtuelle Queue mit der Kapazität \(C_v \le C\), wobei $C$ die Kapazität der Verbindung und $C_v$ die der virtuellen ist. Zu Beginn ist \(C_v = C\). Bei jedem ankommenden Paket wird überprüft, ob der Puffer der virtuellen Queue das Paket aufnehmen könnte. Ist dem so, wird das echte Paket in die tatsächliche Queue eingereiht, ansonsten wird es markiert oder fallen gelassen. Die Kapazität der virtuellen Queue wird ebenfalls bei jedem ankommenden Paket angepasst gemäß der Differentialgleichung 
\begin{displaymath}
\dot{C_v} = \alpha(\gamma C - \lambda)
\end{displaymath}
, wobei $\alpha$ ein Glättungsparameter, $\gamma$ die angestrebte Auslastung der Verbindung und $\lambda$ die Ankunftsrate der Verbindung ist. Das Markieren passiert auf diese Weise aggressiver, also häufiger, wenn die Verbindung ihre gewünschte Übertragungsrate überschreitet und weniger aggressiv, wenn nicht.

Es ist klar, dass in der virtuellen Queue keine Pakete eingereiht werden müssen, lediglich die Länge, also die Kapazität der virtuellen Queue muss ermittelt werden. Die Autoren geben in ihrer Arbeit auch an, wie dieses Verfahren als Algorithmus konkret implementiert werden kann:
\newline Für jedes ankommende Paket DO
\newline \(VQ = max(VQ - C_v(t-s),0)\)
\newline \(if(VQ + b > B)~then\)
\newline \indent Paket markieren
\newline \textit{else}
\newline \indent \(VQ = VQ + b\)
\newline \(C_v = max(min(C_v + \alpha \cdot \gamma \cdot C(t-s),C)-\alpha \cdot b,0)\)
\newline \(s = t\)
\newline wobei $B$ die Puffergröße in Bytes, $s$ die Ankunftszeit des letzten Pakets, $t$ die aktuelle Zeit, $b$ die Paketgröße des aktuellen Pakets in Bytes und $VQ$ die Anzahl an Bytes, die aktuell in der virtuellen Queue sind, ist.

Die Autoren erläutern, dass die algorithmische Komplexität von AVQ in etwa der von RED entspricht. Anstelle der Länge der Queue wird bei AVQ jedoch die Ausnutzung der Queue als Entscheidungskriterium für das Markieren von Paketen verwendet. Für die Verwendung von AVQ müssen der Glättungsparameter $\alpha$ und die gewünschte Ausnutzung $\gamma$ vorab angegeben werden. Diese dienen der Stabilität des Verfahrens. Weiter geben die Autoren an, dass $\gamma$ es den ISP's erlaubt einen Ausgleich zwischen hoher Übertragungsratenausnutzung und kleinen Puffern vorzunehmen. Eine Regel, wie diese Parameter gesetzt werden sollten, befindet sich in der Originalarbeit in \cite{Kunniyur2001}.

\section{Vergleich der vorgestellten Algorithmen}\label{sec:Vergleich}

\subsection{Vergleich BLUE und RED}

In ihrer Arbeit zeigen Feng et al anhand einiger Versuche, dass BLUE im Vergleich zu RED deutlich weniger Pakete fallen lasst \cite{Feng1999}. Für ihren Versuch haben sie ein Netz mithilfe des LBNL Network Simulators \cite{McCanne} simuliert. Für alle Quellen wurde ECN aktiviert, was bedeutet, dass jedes verlorene Paket einen Pufferüberlauf der Queue darstellt. Die Sender wurden zufällig innerhalb der ersten Sekunde gestartet. Die Paketverluste wurden nach 100  Sekunden Simulation und weiteren 100 Sekunden, also insgesamt 200 Sekunden, für die gesamte Simulationszeit gemessen. Die Parameter für RED wurden experimentell bestimmt, $Q_{min}$ betrug immer 20\% der Queuelänge, $Q_{max}$ 80\%. Für BLUE wurde $d_1$ um eine Größenordnung höher als $d_2$ gesetzt. Insgesamt wurden vier Konfigurationen für RED und vier Konfigurationen für den BLUE Algorithmus angewandt und die Ergebnisse verglichen.

Zwischen den Ergebnissen der einzelnen Konfigurationen sowohl bei RED als auch bei BLUE gab es nur minimale Unterschiede, weshalb diese hier zusammengefasst werden. Die Queuelänge für eine Engstelle des Netzes wurde von 100KB bis 1000KB gesetzt, was einer Verzögerung zwischen 17,8ms und 178ms entspricht. Bei 1000 Quellen die gleichzeitig senden, war sowohl bei RED als auch bei BLUE die Verbindung für alle Queuelängen zu 100\% ausgelastet. Die Paketverlustrate betrug bei BLUE für alle Verzögerungen 0\%. Der RED Algorithmus dagegen zeigt bei niedrigen Verzögerungen eine Paketverlusterate von bis zu über 15\%. Diese fällt mit größer werdenden Puffern auf knapp über 0\% bei einer Verzögerung von etwa 100ms, steigt dann aber wieder an auf etwa 5\%. Für das gleiche Experiment mit 4000 Quellen war die Auslastung ebenfalls bei beiden stets bei 100\%. Die Paketverlusterate von RED lag diesmal bei kleinen Verzögerungen bei über 30\%, was mit zunehmenden Queuelängen auf etwa 25\% abfiel. Bei BLUE lag die Verlustrate dagegen zu Beginn bei etwa 15\% und sank auf unter 10\% mit größer werdenden Queues. 

\subsection{Vergleich AVQ und RED}

Analog zu BLUE hat auch Kunniyar in seiner Arbeit zu AVQ Vergleiche des Verfahrens zu anderen AQM Algorithmen gemacht \cite{Kunniyur2001}. Neben RED wurde der Algorithmus auch mit Random Early Marking (REM), dem PI Controller und GKVQ verglichen. Diese werden hier jedoch vernachlässigt, da auch die Algorithmen nicht präsentiert worden sind.

Die beschriebenen Versuche wurden mit dem Simulator \textit{ns-2} durchgeführt. Der Parameter $\gamma$, die angestrebte Auslastung, wurde für AVQ auf den Wert 0,98 gesetzt; der Glättungsparameter $\alpha$ wurde auf 0,15 gesetzt. Die Konfiguration für RED wurde gemäß den Empfehlungen aus \url{http://www.aciri.org/floyd/REDparameters.txt} vorgenommen. Die Queuelänge an der engsten und somit relevanten Stelle der Verbindung betrug 100 Pakete bei einer Paketgröße von 1000 bytes. Für das erste Experiment wurde bei den TCP Verbindungen ECN aktiviert. Jedes verlorene Paket ist folglich ein Zeichen des Pufferüberlaufs. Die Grenzwerte für RED wurden auf \(Q_{min} = 0,37\) und \(Q_{max} = 0,75\) gesetzt. Die durchschnittliche Verzögerung betrug zwischen 30ms und 60ms. Beim Experiment wurde die Paketverlustrate sowie die Auslastung der Verbindung bei unterschiedlicher Anzahl von FTP Verbindungen gemessen, die Anzahl der Verbindungen lag zwischen 20 und 180.

Die Auslastung der Verbindung lag bei RED bei 20 FTP Verbindungen bei knapp unter 90\%. Mit steigender Anzahl an Verbindungen sank diese leicht ab, bewegte sich aber stets zwischen 85\% und 90\%. Für AVQ lag die Auslastung bei 20 Verbindungen bei 95\%. Diese stieg kontinuierlich mit steigenden Verbindungen an und lag bei 180 Verbindungen etwa bei 98\%, der vorab festgelegten angestrebten Auslastung. Die Paketverluste von RED lagen bei 20 Verbindungen etwa bei 0. Mit steigenden FTP Verbindungen stiegen diese in etwa linear auf über 6.000 verlorene Pakete bei 180 Verbindungen. Bei AVQ lagen die Paketverluste für jede Anzahl an Verbindungen bei 0. Der AVQ Algorithmus zeigt für diesen Aufbau eine deutlich besser Leistung mit höherer Auslastung und ohne Paketverluste.

In einem zweiten Experiment wurde ECN deaktiviert. Das heißt, dass sowohl RED als auch AVQ Pakete fallen lassen, um auf einen Stau hinzuweisen. Der Versuchsaufbau ist gleich dem vorherigen mit der Ausnahme, dass für AVQ die angestrebte Auslastung $\gamma$ auf den Wert 1 gesetzt wurde, die Adaptive Virtual Queue also voll genutzt werden soll. Die Anzahl an FTP Verbindungen wurde fest auf 40 gesetzt; dafür wurden TCP short-flows mit 20 Paketen verwendet, deren Ankunftszeit an der kritischen Verbindungsstelle langsam gesteigert wurde. Die Auslastung der Verbindung für AVQ lag konstant bei den angestrebten 100\%. Bei RED lag diese bei 10 ankommenden short-flows pro Sekunde bei etwa 94\% und stieg auf über 99\% bei 50 ankommenden short-flows pro Sekunde. Der Anteil an korrekt zugestellten Paketen, ohne diese einmal fallen zu lassen, lag bei AVQ bei wenigen short-flows bei knapp unter 100\% und sank bis zu knapp unter 98\% bei 50 ankommenden short-flows pro Sekunde. Für RED lag diese Quote anfangs nur bei 87\%, stieg dann aber stark an auf 98\% bei 50 short-flows pro Sekunde und lag damit höher als bei AVQ.

\subsection{Zusammenfassung}

Zu Beginn dieser Arbeit wurde die Notwendigkeit für AQM Algorithmen erläutert. Sie sind notwendig, um aktiv einen Pufferüberlauf der Router in großen Netzen zu verhindern. RED war einer der ersten Algorithmen, der dies zum Ziel hatte. Bereits 1998 wurde von der Network Working Group in RFC 2309 empfohlen, einen AQM Mechanismus zu implementieren, wobei explizit RED als Verfahren vorgeschlagen wurde \cite{Braden1998}. Ebenfalls wurde empfohlen, weiter Forschung in diesem Bereich zu betreiben. Hervorzuheben ist auch, dass die Empfehlung für RED vor der Empfehlung für ECN ausgegeben wurde, welche 1999 in RFC 2481 gegeben wurde \cite{Ramakrishnan1999}.

Aus dieser Forschung sind zahlreiche weitere AQM Algorithmen hervorgegangen. Hier wurden davon die Verfahren BLUE und AVQ vorgestellt und mit RED verglichen. Die Autoren von BLUE zeigen mit ihren Versuchen, dass BLUE eine deutlich geringere Paketverlustrate als RED aufweißt und besonders mit kleinen Puffern noch eine gute Performance zeigt. In den Experimenten zur Leistung von AVQ wurde gezeigt, dass dieses Verfahren eine bessere Auslastung der Verbindung und geringere Paketverluste als RED aufweist. Es ist jedoch zu beachten, dass diese Versuche in kontrollierten Umgebungen durchgeführt wurden mit dem Ziel zu zeigen, dass das jeweilige Verfahren das beste ist.

\section{Ausblick und andere Ansätze}\label{sec:Ausblick}

Neben dem hier vorgestellten Ansatz des Active Queue Managements gibt es auch andere Verfahren, die zum Ziel haben, Überlastungen in Netzen zu vermeiden. In \cite{Tannenbaum2012} werden hierzu zum Beispiel Zugangssteuerung und Routing unter Verkehrsberücksichtigung genannt. Bei der Zugangssteuerung werden nur dann neue, virtuelle Verbindungen aufgebaut, wenn das Netz diese auch verkraften kann. Beim Routing unter Verkehrsberücksichtigung werden Algorithmen angewandt, die im Netz nach möglichst wenig ausgelasteten Pfaden suchen. Diese Ansätze helfen somit auch, die Queues der Router zu entlasten. 

Ein weiterer Ansatz, der den AQM Verfahren schon recht ähnlich ist, wird auch in \cite{Tannenbaum2012} erwähnt: sogenannte Drosselpakete. Hier drosselt der Router den Verkehr nicht, indem er mit den Sendern durch das Markieren oder Fallenlassen von Paketen kommuniziert, sondern er gibt diese Nachricht über die anderen Router an den Sender zurück. Wenn der Router ausgelastet ist, schickt er ein Drosselpaket an den Router, von dem er das letzte Paket erhalten hat. Dieser reagiert sofort darauf, indem er weniger Pakete an diesen Router weiterleitet und seinerseits ein Drosselpaket an den Router vor ihm schickt. So kommt die Nachricht schneller beim Sender an und der Router wird schneller entlastet.

Wie bereits angedeutet sind AQM Algorithmen ein aktuelles Thema der Forschung. Auch in naher Zukunft wird es wohl noch weitere neue Algorithmen geben, die bessere Eigenschaften als die bisherigen aufweisen. Aktuell wichtiger für die Nutzer wäre jedoch, dass AQM Algorithmen auch wirklich auf allen Routern implementiert werden. Wie in dieser Arbeit gezeigt wäre das eine Verbesserung hin zur bestmöglichen Ausnutzung der heute angebotenen Ressourcen.

\bibliographystyle{IEEEtran_de}
\bibliography{IEEEabrv,bibliothek}


\end{document}
